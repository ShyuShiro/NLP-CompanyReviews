{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5c20dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99876bfc",
   "metadata": {},
   "source": [
    "Lets get data request from the webpage and display the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0220e68f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=\"en-US\"><head><meta charSet=\"UTF-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/><link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"https://cdn.trustpilot.net/brand-assets/4.3.0/favicons/favicon.ico\"/><link rel=\"manifest\" href=\"/manifest.json\"/><meta name=\"application-name\" content=\"Trustpilot\"/><meta name=\"theme-color\" content=\"#1c1c1c\"/><link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"https://cdn.trustpilot.net/brand-assets/4.3.0/favicons/a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"https://www.trustpilot.com/review/www.corsair.com?page=10\")\n",
    "r.text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e5508",
   "metadata": {},
   "source": [
    "Text is a giant HTML soup and we're only interested in the reviews on the page\n",
    "- Need to find the div + class structure for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c9d9594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "reviews = soup.find_all(\"div\", {\"class\": \"styles_reviewCardInner__EwDq2\"})\n",
    "stars = soup.find_all(\"div\",\"star-rating_starRating__4rrcf star-rating_medium__iN6Ty\")\n",
    "print(len(reviews))\n",
    "print(len(stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374ce7b",
   "metadata": {},
   "source": [
    "There's 4 more stars than reviews .. why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19f96e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"TrustScore 2 out of 5\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-2.svg\"/>\n",
      "<img alt=\"Rated 3 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-3.svg\"/>\n",
      "<img alt=\"Rated 4 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-4.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 3 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-3.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 3 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-3.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 4 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-4.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 5 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-5.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 5 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-5.svg\"/>\n",
      "<img alt=\"\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-4.svg\"/>\n",
      "<img alt=\"\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.5.svg\"/>\n",
      "<img alt=\"\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.5.svg\"/>\n"
     ]
    }
   ],
   "source": [
    "for star in stars:\n",
    "    print(star.find_all(\"img\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61921609",
   "metadata": {},
   "source": [
    "The first entry is the company's overall score while the last 3 entries are scores of other companies in the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "213c7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"Rated 3 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-3.svg\"/>\n",
      "<img alt=\"Rated 4 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-4.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 3 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-3.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 3 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-3.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 4 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-4.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 5 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-5.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 1 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-1.svg\"/>\n",
      "<img alt=\"Rated 5 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-5.svg\"/>\n"
     ]
    }
   ],
   "source": [
    "stars = stars[1:-3] #Fixed\n",
    "for star in stars:\n",
    "    print(star.find_all(\"img\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a21be6",
   "metadata": {},
   "source": [
    "Above I've used Chrome's Object Painter to discover the div container and its class name for reviews ... here's an example of how to get the review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0b78e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"star-rating_starRating__4rrcf star-rating_medium__iN6Ty\"><img alt=\"Rated 3 out of 5 stars\" src=\"https://cdn.trustpilot.net/brand-assets/4.1.0/stars/stars-3.svg\"/></div>\n",
      "\n",
      "Rating is: 3\n"
     ]
    }
   ],
   "source": [
    "#Show a random review\n",
    "import numpy as np\n",
    "rand = np.random.choice(len(reviews))\n",
    "\n",
    "#How to get the star rating\n",
    "print(stars[rand])\n",
    "print(\"\\nRating is: \"+stars[rand].find_all(\"img\")[0]['src'][-5]) #Find the img, access the first image (only image), go to src tag, grab the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a53f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAJames4 reviewsGBApr 19, 2023Super Fast Service.Ordered at 12:52, dispatched at 13:45 and arrived at 09:42 the next day! Can't fault at all.Date of experience: April 18, 2023Reply from CorsairMay 9, 2023Thank you James, for your 5* response and for being a valued Corsair customer!\n"
     ]
    }
   ],
   "source": [
    "#Show a random review\n",
    "import numpy as np\n",
    "rand = np.random.choice(len(reviews))\n",
    "print(reviews[rand].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63085c07",
   "metadata": {},
   "source": [
    "Need to create a function to gather the following from the review:\n",
    "- The date of the review (For timeline chart purposes)\n",
    "- Body of the review (Text not including the name or location)\n",
    "> Subsequently use NLP filter the text for a word cloud graphic\n",
    "- Location of the Reviewer (Ex: US vs AT vs GB)\n",
    "> Subsequently need to discipher these 2 letter codes because \"AT\" is Austria and \"GB\" is Great Britain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5c8cd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 17, 2023\n",
      "May 08, 2023\n",
      "August 09, 2023\n",
      "December 03, 2023\n",
      "March 30, 2022\n",
      "February 24, 2023\n",
      "September 28, 2022\n",
      "July 01, 2022\n",
      "August 08, 2023\n",
      "February 17, 2022\n",
      "April 01, 2022\n",
      "August 24, 2022\n",
      "July 24, 2023\n",
      "September 18, 2023\n",
      "April 05, 2022\n",
      "April 18, 2023\n",
      "January 02, 2024\n",
      "January 04, 2024\n",
      "May 24, 2022\n",
      "January 09, 2023\n"
     ]
    }
   ],
   "source": [
    "def find_date(txt):\n",
    "    '''\n",
    "    Function to find the date -- Use Date of Experience\n",
    "    '''\n",
    "    try:\n",
    "        sub_str = txt.find_all(\"p\")[1].get_text() #\"Date of experience: January 02, 2024\"\n",
    "        start = re.search(\"Date of experience:\",sub_str).span()[1]\n",
    "        return sub_str[start+1:]\n",
    "    except:\n",
    "        start = re.search(\"Date of experience: \",txt.get_text()).span()[1]\n",
    "        try: #In the event there's a Company reply\n",
    "            end = re.search(\"Reply\",txt.get_text()).span()[0]\n",
    "            return txt.get_text()[start:end] #Only get the date\n",
    "        except:\n",
    "            return txt.get_text()[start:] #Else this is the end of the string already\n",
    "\n",
    "for i in reviews: #Test\n",
    "    print(find_date(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118488b",
   "metadata": {},
   "source": [
    "This kind of convenient -- Replies from Corsair would appear after the \"Date of experience\" text. Lets update the `find_date`function and then figure out how to extract replies from the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40f8eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Reply\n",
      "No Reply\n",
      "Hi James,We apologize for the inconvenience you've experienced with our customer service and shipping. We're here to help resolve any issues you've encountered. Please provide us with more details about your situation, and we'll do our best to assist you and improve your overall experience. Your feedback is important!\n",
      "No Reply\n",
      "Thank you for your recent order, Jorgen. We apologize for any shipping delays. UPS is estimating delivery very soon. If you are not satisfied with the item you may return if for a full refund within 30 days.\n",
      "No Reply\n",
      "No Reply\n",
      "Hello, we apologize for the delay in completing your warranty exchange. Our team will reach out to you on your existing support ticket with an update.\n",
      "Hi David,Combining two separate RAM kits, even if they are of the same model, can lead to compatibility issues due to manufacturing variations in timing, voltage, memory ICs, and configuration. These differences may cause instability, crashes, or boot failures. I apologize for any inconvenience that this may have caused. For future reference, more information regarding mixing memory kits can be found in our help center - https://help.corsair.com/hc/en-usThank you for your feedback!\n",
      "Hello Katherine. We are reviewing your support claim case, and will contact you on your existing ticket shortly.\n",
      "Please contact our support team at help.corsair.com, or if you already have an existing support ticket starting with \"200\" let us know the number so we can investigate.\n",
      "Hello Cooper. If your products are still within warranty coverage please contact our team at help.corsair.com for assistance with your claim.\n",
      "Hello, We're regret to hear about your experience with our iCUE software. We apologize for any inconvenience it may have caused you. Our team is here to help address any issues you're facing and provide assistance to improve your overall experience. Please feel free to share more details about your concerns, and we'll do our best to assist you in resolving them. Thank you for your feedback!\n",
      "No Reply\n",
      "Thank you for your review. Corsair offers a 30 day money back guarantee. If you are not satisfied you may return the computer system for a refund.\n",
      "Thank you James, for your 5* response and for being a valued Corsair customer!\n",
      "No Reply\n",
      "No Reply\n",
      "No Reply\n",
      "No Reply\n"
     ]
    }
   ],
   "source": [
    "def find_reply(txt):\n",
    "    '''\n",
    "    Function to find replies if there are any from a representative\n",
    "    '''\n",
    "    try:\n",
    "        return txt.find_all(\"p\")[3].get_text()\n",
    "    except:\n",
    "        return \"No Reply\"\n",
    "\n",
    "for i in reviews: #Test\n",
    "    print(find_reply(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eaf234",
   "metadata": {},
   "source": [
    "Now find the geography which is just after the word \"review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b50dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SU\n",
      "DE\n",
      "GB\n",
      "SD\n",
      "GB\n",
      "GB\n",
      "US\n",
      "BJ\n",
      "GB\n",
      "US\n",
      "CA\n",
      "UA\n",
      "AJ\n",
      "GB\n",
      "SA\n",
      "GB\n",
      "GB\n",
      "US\n",
      "SM\n",
      "AU\n"
     ]
    }
   ],
   "source": [
    " def find_geo(txt):\n",
    "    '''\n",
    "    Find geography tag for the review\n",
    "    '''\n",
    "    txt = txt.get_text()\n",
    "    start = re.search(\"review\",txt).span()[1] #Not plural because 1 review vs 10 review**s**\n",
    "    return txt[start+1:start+3]\n",
    "\n",
    "for i in reviews: #Test\n",
    "    print(find_geo(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be825e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrible experience/ belated great customer service Terrible experience! Ordered a $4200 build kit on 4/29 (website says ships in 1-2 weeks). 2 days later it changes to mid may, 2 days later end on May. The next day they took the ability to purchase it off the website. So at this point I'm thinking thank God I ordered when I did. May 16/17th rolls around and corsair has website maintenance. Check my order the next day and it's gone! Call customer support, they can't find it.  Finally come back after a hour and say they found it no problem will ship out on 5/26. Not happy but could be worse, so ok I wait... 26th rolls around, and still nothing. Check my order and they just replaced it on 5/18.. call customer service again and am told that they don't know when it will ship and I'm just out of luck bc the i9-13900ks is backordered. Worst experience ever.. I was lied to multiple times and would have nvr purchased if I knew it would be a month down the line with no end in sight! Plus the fact that I lost my spot in line due to their incompetence! Avoid there website like the plague!Edit: I received my order on 6/1. Everything was exactly as it should besides when they cloned the OS onto my 2tb nvme they used a 1tb drive which partitioned 1tb off the end. Not a huge deal but seemed a little sloppy. But I can't complain bc they sent me 2 extra 2tb nvme drives for my wait! Took me like 7 tries to get a good customer service representative but once I did she went above and beyond to help and keep me informed of what was happening.\n"
     ]
    }
   ],
   "source": [
    "def find_body(txt):\n",
    "    '''\n",
    "    Function to find body text\n",
    "    '''\n",
    "#    search = re.finditer(r\"\\d{1}, \\d{4}\",txt)\n",
    "#    end = re.search(\"Date of experience:\",txt).span()[0]\n",
    "#    idxs = [i.span() for i in search]\n",
    "#    #Find the text between the review date and date of experience\n",
    "#    try: #The reviews which are written recently say \"3 hours ago\"\n",
    "#        #find the \"ago\" text\n",
    "#        start = re.search(\" ago\",txt).span()[1]\n",
    "#        return txt[start:end]\n",
    "#    except:\n",
    "#        return txt[idxs[0][1]:end]\n",
    "    title = txt.find_all(\"h2\")[0].get_text()\n",
    "    body = txt.find_all(\"p\")[0].get_text()\n",
    "    return title  + \" \" + body\n",
    "    \n",
    "for i in reviews: #Test\n",
    "    print(find_body(i))\n",
    "    break #dont show all of them in this case ... too much text block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4be933",
   "metadata": {},
   "source": [
    "Now that functions exist for Date, Replies, Region, and Body text.\n",
    "\n",
    "This is only the first page of reviews -- Need to write some code to parse all the pagenation and save the reviews to a .csv file because I don't want to keep polling the webpage if its not necessary!\n",
    "\n",
    "After that, the next steps will be to focus on :\n",
    "- NLP to remove filler words and generate sentiment analysis\n",
    "- Dictionary of all the regions so we can make an analysis based on region!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b73dc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "Good Very strange seeing it getting 2star rating any time I've had issues they helped me super quick and always sent me out parts that needed if broken.tje core nodes can be a bit better but in general there are very good.\n"
     ]
    }
   ],
   "source": [
    "def grab_reviews(URL):\n",
    "    dates = [] #Blank\n",
    "    geos = [] #Blank\n",
    "    replies = [] #Blank\n",
    "    reviews = [] #Blank\n",
    "    stars = [] #Blank\n",
    "    for count in range(1,41):\n",
    "        print(count)\n",
    "        if count == 1:\n",
    "            r = requests.get(URL)\n",
    "        else:\n",
    "            r = requests.get(URL+\"?page=\"+str(count))\n",
    "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "        review = soup.find_all(\"div\", {\"class\": \"styles_reviewCardInner__EwDq2\"})\n",
    "        star = soup.find_all(\"div\",\"star-rating_starRating__4rrcf star-rating_medium__iN6Ty\")\n",
    "        star = star[1:-3] #Remove first (company overall) and last 3 (other company) stars\n",
    "#        #Debug\n",
    "#        print(len(review))\n",
    "#        print(len(star))\n",
    "        time.sleep(1) #Pause just to not overload the webpage\n",
    "\n",
    "        #Process the data\n",
    "        for i in review:\n",
    "            dates.append(find_date(i))\n",
    "            geos.append(find_geo(i))\n",
    "            reviews.append(find_body(i))\n",
    "            replies.append(find_reply(i))\n",
    "        for j in star:\n",
    "            stars.append(j.find_all(\"img\")[0]['src'][-5])\n",
    "    return [dates,geos,reviews,replies,stars]\n",
    "\n",
    "data = grab_reviews(\"https://www.trustpilot.com/review/www.corsair.com\")\n",
    "print(data[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "022563d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block to save the data so I don't have to keep polling the webpage\n",
    "import pickle\n",
    "\n",
    "with open('reviews.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6adc07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Good Very strange seeing it getting 2star rating any time I've had issues they helped me super quick and always sent me out parts that needed if broken.tje core nodes can be a bit better but in general there are very good.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Block to load the pickled data so we can checkpoint here instead\n",
    "import pickle\n",
    "with open('reviews.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    dat = pickle.load(f)\n",
    "dat[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c91fd9",
   "metadata": {},
   "source": [
    "# NLP Work\n",
    "Now the focus will be to perform some NLP work to\n",
    "- Tokenize words (Ie: Grab individual words)\n",
    "- Normalize Text (Ie: All lowercase letters)\n",
    "- Clean up filler words (Ex: in, on, I)\n",
    "- Do some Sentiment Analysis on each review\n",
    "- Frequency analysis (Ie: What is most commonly said about the company?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "973d68ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'very', 'strange', 'seeing', 'it', 'getting', '2star', 'rating', 'any', 'time', 'i', \"'ve\", 'had', 'issues', 'they']\n"
     ]
    }
   ],
   "source": [
    "[dates,geos,reviews,replies,stars] = dat #Segment the data out into the individual lists\n",
    "txt = reviews[0] #Lets work with the first review\n",
    "\n",
    "print([i.lower() for i in nltk.word_tokenize(txt)][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be971b6f",
   "metadata": {},
   "source": [
    "Problem: The conjunction word \"I have/had\" became \"i\" and \"'ve\" but should have been \"i've\"\n",
    "\n",
    "Need to utilize a different tokenizer -- Try tweet_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "121e2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'Very', 'strange', 'seeing', 'it', 'getting', '2star', 'rating', 'any', 'time', \"I've\", 'had', 'issues', 'they', 'helped']\n",
      "['needed', 'if', 'broken.tje']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(txt)[:15])\n",
    "print(tokenizer.tokenize(txt)[25:28])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf423d",
   "metadata": {},
   "source": [
    "This works, but there's 2 problems that I would like to fix:\n",
    "- The title of the review (In this case: \"Good\") has no space between it and the start of the review\n",
    "- Human error resulted in \"broken.tje\"\n",
    "> I can't fix the word \"tje\" to \"the\" unless I just manually fix that, but it should hopefully be removed by NLTK through an English Dictionary filter <br>\n",
    "> There is no space after the period, that can be added with a split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "954fda74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'very', 'strange', 'seeing', 'it', 'getting', '2star', 'rating', 'any', 'time', \"i've\", 'had', 'issues', 'they', 'helped', 'me', 'super', 'quick', 'and', 'always', 'sent', 'me', 'out', 'parts', 'that', 'needed', 'if', 'broken', 'tje', 'core', 'nodes', 'can', 'be', 'a', 'bit', 'better', 'but', 'in', 'general', 'there', 'are', 'very', 'good']\n"
     ]
    }
   ],
   "source": [
    "fixed_txt = ' '.join(txt.split('.')) #This will adjust the text to replace periods with spaces\n",
    "txt_token = [i.lower() for i in tokenizer.tokenize(fixed_txt)]\n",
    "print(txt_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17278444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'strange',\n",
       " 'seeing',\n",
       " 'getting',\n",
       " '2star',\n",
       " 'rating',\n",
       " 'time',\n",
       " \"i've\",\n",
       " 'issues',\n",
       " 'helped',\n",
       " 'super',\n",
       " 'quick',\n",
       " 'always',\n",
       " 'sent',\n",
       " 'parts',\n",
       " 'needed',\n",
       " 'broken',\n",
       " 'tje',\n",
       " 'core',\n",
       " 'nodes',\n",
       " 'bit',\n",
       " 'better',\n",
       " 'general',\n",
       " 'good']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "removed_stopwords = [i for i in txt_token if i not in stopwords]\n",
    "removed_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af6afb",
   "metadata": {},
   "source": [
    "The word \"Good\" is used multiple times -- We need unique words and how often they appear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3820f988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'good': 2, 'strange': 1, 'seeing': 1, 'getting': 1, '2star': 1, 'rating': 1, 'time': 1, \"i've\": 1, 'issues': 1, 'helped': 1, ...})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cln_text = nltk.FreqDist(removed_stopwords)\n",
    "cln_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4658b37",
   "metadata": {},
   "source": [
    "Future self --- Words like \"Customer\",\"Support\",\"Service\" are great words to keep.\n",
    "\n",
    "Words like \"Corsair\", \"one\", \"be\", \"would\" however are not ... lets extend this stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49df1a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some words from the abc corpus\n",
      " ['denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite']\n"
     ]
    }
   ],
   "source": [
    "common_words = nltk.corpus.abc.words()\n",
    "#Find all the Corpus options here and pick the most appropriate (or load multiple)\n",
    "#https://www.nltk.org/howto/corpus.html#plaintext-corpora\n",
    "print(\"Here are some words from the abc corpus\\n\",common_words[10:20])\n",
    "stopwords.extend([w.lower() for w in common_words]) #Generically add the Corpus \n",
    "stopwords.extend(['corsair','cant',\"im\",\"wont'\",\"wont\",\"corsairs\",\"didnt\",\"lol\",\n",
    "                 \"thats\",\"nope\",\"deres\",\"ww\",\"kinda\",'og','anyways','cw',\"wasnt'\",\n",
    "                  \"til\",\"min\",\"fra\",\"vil\",\"gb\",\"er\",\"skal\",\"sh\",\"psus\",\"med\",\n",
    "                  \"på\",\"alot\",\"alright\",\"wasnt\",\"trustpilot\",\"reddit\",\"nzxt\",\"newegg\",\n",
    "                  \"ikke\",\"\"\n",
    "                 ]) #Remove these words as well based on future lookback on words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a583b",
   "metadata": {},
   "source": [
    "Put all together ... this is a funciton to do all the text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45489ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 1 samples and 1 outcomes>\n",
      "\n",
      "CPU times: total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def clean_text(txt,filter_words=stopwords):\n",
    "    fixed_txt = ' '.join(txt.split('.')) #This will adjust the text to replace periods with spaces\n",
    "    txt_token = [i.lower() for i in tokenizer.tokenize(fixed_txt)]\n",
    "    removed_stopwords = [i for i in txt_token if i not in filter_words]\n",
    "    words_only = [i for i in removed_stopwords if i.isalpha()] #Added to remove [\",\",\"...\"] and [\"1\",\"50\"] entries\n",
    "    dist = nltk.FreqDist(words_only)\n",
    "    return dist\n",
    "\n",
    "tokenized_reviews = []\n",
    "for review in reviews:\n",
    "    tokenized_reviews.append(clean_text(review))\n",
    "print(tokenized_reviews[0])\n",
    "\n",
    "tokenized_replies = []\n",
    "for reply in replies:\n",
    "    if reply != \"No Reply\":\n",
    "        tokenized_replies.append(clean_text(reply))\n",
    "    else:\n",
    "        tokenized_replies.append(\"\")\n",
    "print(tokenized_replies[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c23e8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Geo</th>\n",
       "      <th>Review</th>\n",
       "      <th>Reply</th>\n",
       "      <th>Token_Review</th>\n",
       "      <th>Token_Reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>February 26, 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>UF</td>\n",
       "      <td>The absolute worst customer experience I have ...</td>\n",
       "      <td>No Reply</td>\n",
       "      <td>{'rma': 1, 'blatantly': 1}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>October 03, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>SU</td>\n",
       "      <td>Corsair sent my product out anyway, 12 days af...</td>\n",
       "      <td>Hello Jordon,We apologize for the delays in ca...</td>\n",
       "      <td>{'atrocious': 1, 'guesses': 1, 'mindless': 1, ...</td>\n",
       "      <td>{'jordon': 1, 'apologize': 1, 'canceling': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>March 07, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>BM</td>\n",
       "      <td>I tried calling their phone support who… I tri...</td>\n",
       "      <td>Hi Anon,Can you give me your ticket number so ...</td>\n",
       "      <td>{'troubleshoot': 1, 'updating': 1}</td>\n",
       "      <td>{'anon': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>July 25, 2021</td>\n",
       "      <td>1</td>\n",
       "      <td>SU</td>\n",
       "      <td>WORST EVER! 2 tickets created for same reason2...</td>\n",
       "      <td>Hello Allen. We apologize for the delay in pro...</td>\n",
       "      <td>{'rma': 5, 'payed': 2, 'vengeance': 1, 'anytim...</td>\n",
       "      <td>{'apologize': 1, 'warranty': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>August 01, 2022</td>\n",
       "      <td>2</td>\n",
       "      <td>SU</td>\n",
       "      <td>Fake Warranty I received the RGB Elite fans as...</td>\n",
       "      <td>Hello Jason. Corsair's published limited warra...</td>\n",
       "      <td>{'warranty': 8, 'rgb': 1, 'technicalties': 1, ...</td>\n",
       "      <td>{'warranty': 1, 'authorized': 1, 'reseller': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Rating Geo  \\\n",
       "749  February 26, 2018      1  UF   \n",
       "599   October 03, 2019      1  SU   \n",
       "670     March 07, 2019      1  BM   \n",
       "264      July 25, 2021      1  SU   \n",
       "136    August 01, 2022      2  SU   \n",
       "\n",
       "                                                Review  \\\n",
       "749  The absolute worst customer experience I have ...   \n",
       "599  Corsair sent my product out anyway, 12 days af...   \n",
       "670  I tried calling their phone support who… I tri...   \n",
       "264  WORST EVER! 2 tickets created for same reason2...   \n",
       "136  Fake Warranty I received the RGB Elite fans as...   \n",
       "\n",
       "                                                 Reply  \\\n",
       "749                                           No Reply   \n",
       "599  Hello Jordon,We apologize for the delays in ca...   \n",
       "670  Hi Anon,Can you give me your ticket number so ...   \n",
       "264  Hello Allen. We apologize for the delay in pro...   \n",
       "136  Hello Jason. Corsair's published limited warra...   \n",
       "\n",
       "                                          Token_Review  \\\n",
       "749                         {'rma': 1, 'blatantly': 1}   \n",
       "599  {'atrocious': 1, 'guesses': 1, 'mindless': 1, ...   \n",
       "670                 {'troubleshoot': 1, 'updating': 1}   \n",
       "264  {'rma': 5, 'payed': 2, 'vengeance': 1, 'anytim...   \n",
       "136  {'warranty': 8, 'rgb': 1, 'technicalties': 1, ...   \n",
       "\n",
       "                                           Token_Reply  \n",
       "749                                                     \n",
       "599  {'jordon': 1, 'apologize': 1, 'canceling': 1, ...  \n",
       "670                                        {'anon': 1}  \n",
       "264                    {'apologize': 1, 'warranty': 1}  \n",
       "136    {'warranty': 1, 'authorized': 1, 'reseller': 1}  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([dates,stars,geos,reviews,replies,tokenized_reviews,tokenized_replies]).T\n",
    "df.columns = [\"Date\",\"Rating\",\"Geo\",\"Review\",\"Reply\",\"Token_Review\",\"Token_Reply\"]\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25983a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GB' 'R2' 'AT' 'DK' 'BA' 'FI' 'UU' 'NO' 'US' 'BE' 'AJ' 'SJ' 'NL' 'IE'\n",
      " 'DE' 'SA' 'BJ' 'BD' 'IT' 'PU' 'SM' 'SU' 'EM' 'SN' 'BM' 'LD' 'SO' 'UM'\n",
      " 'AU' 'AA' 'FR' 'BY' 'UJ' 'EG' 'GR' 'BF' 'LJ' 'PL' 'BU' 'AN' 'SS' 'CA'\n",
      " 'SF' 'PR' 'UA' 'HJ' 'SD' 'AM' 'TU' 'HU' 'BN' 'KM' 'EE' 'ZA' 'SE' 'AO'\n",
      " 'BO' 'BS' 'LA' 'CH' 'TJ' 'PT' 'DM' 'TA' 'CY' 'TR' 'AF' 'GU' 'ON' 'AE'\n",
      " 'NU' 'ES' 'ID' 'NS' 'SK' 'EU' 'IL' 'RJ' 'GM' 'BR' 'WU' 'EJ' 'GE' 'GG'\n",
      " 'LT' 'AR' 'YU' 'KU' 'HR' 'UF' 'RO']\n"
     ]
    }
   ],
   "source": [
    "regions = df[\"Geo\"].unique()\n",
    "print(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87311fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_dict = {'GB': \"Great Britain\", 'AT':'Austria', 'DK':'Denmark',\n",
    "            'BA':'Bosnia', 'FI':'Finland',\n",
    "            'UU':'Australia', #Manual check revealed to be AU\n",
    "            'NO':'Norway', 'US':'United States','BE':'Belgium',\n",
    "            'AJ':'Canada', #Manual check revealed to be CA\n",
    "            'SJ':'Svalbard','NL':\"Netherlands\", 'IT':'Italy',\n",
    "            'IE':'Ireland','DE':'Germany','SA':'Saudi Arabia','BJ':'Benin',\n",
    "            'PU':'Japan', #Manual check revealed to be JP\n",
    "            'SM':'San Marino', 'SU':'Suriname', 'BD':'Bangladesh',\n",
    "            'EM':'Germany', #Manual check revealed to be DE\n",
    "            'LD':'Netherlands', #Manual check revealed to be NL\n",
    "            'BM':'Bermuda','SN':'Senegal','SO':'Somalia',\n",
    "            'AA':'Canada', #Manual check revealed to be CA\n",
    "            'UM':'United States', #Rename as US\n",
    "            'FR':'France', 'BY':'Belarus', 'AU':'Australia',\n",
    "            'UJ':'Australia', #Manual check revealed to be AU\n",
    "            'GR':'Greece','BF':'Burkina Faso', 'CA':\"Canada\",\n",
    "            'PL':'Poland', 'EG':'Egypt', \n",
    "            'LJ':'Netherlands',#Manual check revealed to be NL\n",
    "            'BU':'Great Britain', #Manual check revealed to be GB\n",
    "            'AN':'Canada', #Manual check revealed to be CA\n",
    "            'SS':'Sudan', #Rename to Sudan\n",
    "            'SF':'Great Britain', #Manual check revealed to be GB\n",
    "            'PR':'Puerto Rico', 'UA':'Ukraine', 'SD':'Sudan',\n",
    "            'HJ':'Switzerland', #Manual check revealed to be CH\n",
    "            'AM':'Armenia', 'BN':'Brunei Darussalam',\n",
    "            'TU':'Portugal', #Manual check revealed to be PT\n",
    "            'KM':'Comoros', 'HU':'Hungary', 'EE':'Estonia',\n",
    "            'ZA':'South Africa', 'SE':'Sweden', 'AO':'Angola',\n",
    "            'BO':'Bolivia', 'BS':'Bahamas', 'LA':\"Lao People's Democratic Republic\",\n",
    "            'CH':'Switzerland', 'TJ':'Tajikistan', 'PT':'Portugal',\n",
    "            'DM':'Dominica', 'CY':'Cyprus', \n",
    "            'TA':'Portugal', #Manual check revealed to be PT \n",
    "            'TR':'Turkey', 'AF':'Afghanistan', 'GU':'Guam',\n",
    "            'ON':'Bolivia', #Manual check revealed to be BO\n",
    "            'AE':'United Arab Emirates', 'NU':'Niue',\n",
    "            'ES':'Spain', 'ID':'Indonesia', \n",
    "            'NS':\"India\", #Manual check reveleaed to be IN\n",
    "            'SK':'Slovakia', 'IL':'Israel',\n",
    "            'RJ':'Puerto Rico', #Manual check\n",
    "            'EU':'Ireland', #Manual check\n",
    "            'GM':'Gambia', 'BR':'Brazil', 'GE':'Georgia', \n",
    "            'WU':'Taiwan', #Manual check\n",
    "            'EJ':\"Sweden\", #Manual check\n",
    "            'GG':'Guernsey', 'LT':'Lithuania','AR':'Argentina', \n",
    "            'YU':'Malaysia', #Manual check\n",
    "            'KU':'Kuwait', 'HR':'Croatia','RO':'Romania',\n",
    "            'UF':'Australia'} #Manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "569a8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Geo'] = df['Geo'].map(geo_dict) #Write the changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7e95a",
   "metadata": {},
   "source": [
    "Save the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b000bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "721c1586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Geo</th>\n",
       "      <th>Review</th>\n",
       "      <th>Reply</th>\n",
       "      <th>Token_Review</th>\n",
       "      <th>Token_Reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>July 03, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>Suriname</td>\n",
       "      <td>I ordered a Water Cooling system a week… I ord...</td>\n",
       "      <td>Hello Michelle,I do apologize for the hold tim...</td>\n",
       "      <td>{'realized': 1}</td>\n",
       "      <td>{'apologize': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>February 25, 2016</td>\n",
       "      <td>5</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>excellent Very good service in Europe. My Powe...</td>\n",
       "      <td>No Reply</td>\n",
       "      <td>{'psu': 1, 'rma': 1}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>December 08, 2022</td>\n",
       "      <td>1</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>Corsair misleading customers over warranty? Co...</td>\n",
       "      <td>No Reply</td>\n",
       "      <td>{'warranty': 3, 'ddr': 1, 'rma': 1, 'honor': 1...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>February 07, 2022</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Britain</td>\n",
       "      <td>Very Poor Customer Service Experience Was give...</td>\n",
       "      <td>Hello Thomas. Thank you for your recent order,...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'apologize': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>December 27, 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>Suriname</td>\n",
       "      <td>Awful awful customer service Customer service ...</td>\n",
       "      <td>Hi Brian,I would love to review your chat tran...</td>\n",
       "      <td>{'twists': 1, 'chats': 1, 'arrangememnts': 1}</td>\n",
       "      <td>{'transcripts': 1, 'chats': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Rating            Geo  \\\n",
       "638      July 03, 2019      1       Suriname   \n",
       "786  February 25, 2016      5    Netherlands   \n",
       "69   December 08, 2022      1     Bangladesh   \n",
       "137  February 07, 2022      1  Great Britain   \n",
       "695  December 27, 2018      1       Suriname   \n",
       "\n",
       "                                                Review  \\\n",
       "638  I ordered a Water Cooling system a week… I ord...   \n",
       "786  excellent Very good service in Europe. My Powe...   \n",
       "69   Corsair misleading customers over warranty? Co...   \n",
       "137  Very Poor Customer Service Experience Was give...   \n",
       "695  Awful awful customer service Customer service ...   \n",
       "\n",
       "                                                 Reply  \\\n",
       "638  Hello Michelle,I do apologize for the hold tim...   \n",
       "786                                           No Reply   \n",
       "69                                            No Reply   \n",
       "137  Hello Thomas. Thank you for your recent order,...   \n",
       "695  Hi Brian,I would love to review your chat tran...   \n",
       "\n",
       "                                          Token_Review  \\\n",
       "638                                    {'realized': 1}   \n",
       "786                               {'psu': 1, 'rma': 1}   \n",
       "69   {'warranty': 3, 'ddr': 1, 'rma': 1, 'honor': 1...   \n",
       "137                                                 {}   \n",
       "695      {'twists': 1, 'chats': 1, 'arrangememnts': 1}   \n",
       "\n",
       "                        Token_Reply  \n",
       "638                {'apologize': 1}  \n",
       "786                                  \n",
       "69                                   \n",
       "137                {'apologize': 1}  \n",
       "695  {'transcripts': 1, 'chats': 1}  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load df\n",
    "import pickle\n",
    "with open('df.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    df = pickle.load(f)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ba79d",
   "metadata": {},
   "source": [
    "This table will be useful for Tableau in determining\n",
    "- the number of reviews by country\n",
    "- the rating by country\n",
    "- which country has the most support from the company\n",
    "- trend line of the company's review history\n",
    "\n",
    "What this wont be useful for unfortunately is\n",
    "- Wordcloud of all the text --- The review/reply data is individual right now, i need a pool of that data\n",
    "- the most common words used for a Good/Bad review\n",
    "- the most common complaint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fdc52",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55c93f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the tokenized reviews added together\n",
    "pool,good_pool,bad_pool = nltk.FreqDist(),nltk.FreqDist(),nltk.FreqDist()\n",
    "for i in range(df.shape[0]):\n",
    "    review = df.iloc[i]['Token_Review']\n",
    "    rating = float(df.iloc[i][\"Rating\"])\n",
    "    \n",
    "    pool+= review #Add to general pool\n",
    "    if rating >=2.5:\n",
    "        good_pool += review #Only good reviews\n",
    "    else:\n",
    "        bad_pool += review #Only bad reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bd9b47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('psu', 85),\n",
       " ('rma', 83),\n",
       " ('warranty', 77),\n",
       " ('headset', 65),\n",
       " ('edit', 36),\n",
       " ('rgb', 35),\n",
       " ('icue', 20),\n",
       " ('refurbished', 20),\n",
       " ('aio', 19),\n",
       " ('dhl', 18)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_pool.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12ad2b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rma', 218),\n",
       " ('warranty', 181),\n",
       " ('headset', 108),\n",
       " ('psu', 101),\n",
       " ('rgb', 83),\n",
       " ('icue', 68),\n",
       " ('edit', 44),\n",
       " ('receipt', 35),\n",
       " ('invoice', 28),\n",
       " ('dhl', 28)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_pool.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f0e99",
   "metadata": {},
   "source": [
    "Looking at these two most_common lists ... it seems they're nearly the same. Both good and bad reviews praise/comaplin about Corsair, Customer service, and support.\n",
    "\n",
    "Unfortunately, there seems to be quite a bit more cleaning work that I need to do because words like \"one\", \"would\", \"order\", \"get\" ... none of these hold any substance what made the review particularly good nor bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a22abe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Geo</th>\n",
       "      <th>Review</th>\n",
       "      <th>Reply</th>\n",
       "      <th>Token_Review</th>\n",
       "      <th>Token_Reply</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>February 04, 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Britain</td>\n",
       "      <td>Well here we go again having to post on… Well ...</td>\n",
       "      <td>Hi Mathew,Jason requested the tilt legs for yo...</td>\n",
       "      <td>{'rubbers': 1, 'mins': 1}</td>\n",
       "      <td>{'mathew': 1}</td>\n",
       "      <td>0.9008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>August 21, 2020</td>\n",
       "      <td>4</td>\n",
       "      <td>Great Britain</td>\n",
       "      <td>RAM Lifetime Warranty Just made a claim on ram...</td>\n",
       "      <td>Hello Robert. Thank you for your review, and I...</td>\n",
       "      <td>{'warranty': 4, 'receipt': 1, 'troublesome': 1...</td>\n",
       "      <td>{'warranty': 2}</td>\n",
       "      <td>-0.7243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>May 06, 2021</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Worst customer service ever! On March 16th, 20...</td>\n",
       "      <td>Corsair strives to provide the best customer s...</td>\n",
       "      <td>{'vengence': 1, 'gpus': 1, 'gpu': 2, 'setup': ...</td>\n",
       "      <td>{'regretfully': 1}</td>\n",
       "      <td>0.9961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>June 21, 2021</td>\n",
       "      <td>5</td>\n",
       "      <td>Svalbard</td>\n",
       "      <td>I love my corsair crystal computer… I love my ...</td>\n",
       "      <td>Thank you for the review and for the support, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.9509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>March 21, 2021</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Britain</td>\n",
       "      <td>Never shopping here again A simple return for ...</td>\n",
       "      <td>Hello LonLon. We apologize for the delays with...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'lonlon': 1, 'apologize': 1, 'rma': 1}</td>\n",
       "      <td>-0.8172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Rating            Geo  \\\n",
       "681  February 04, 2019      1  Great Britain   \n",
       "412    August 21, 2020      4  Great Britain   \n",
       "289       May 06, 2021      1      Australia   \n",
       "272      June 21, 2021      5       Svalbard   \n",
       "308     March 21, 2021      1  Great Britain   \n",
       "\n",
       "                                                Review  \\\n",
       "681  Well here we go again having to post on… Well ...   \n",
       "412  RAM Lifetime Warranty Just made a claim on ram...   \n",
       "289  Worst customer service ever! On March 16th, 20...   \n",
       "272  I love my corsair crystal computer… I love my ...   \n",
       "308  Never shopping here again A simple return for ...   \n",
       "\n",
       "                                                 Reply  \\\n",
       "681  Hi Mathew,Jason requested the tilt legs for yo...   \n",
       "412  Hello Robert. Thank you for your review, and I...   \n",
       "289  Corsair strives to provide the best customer s...   \n",
       "272  Thank you for the review and for the support, ...   \n",
       "308  Hello LonLon. We apologize for the delays with...   \n",
       "\n",
       "                                          Token_Review  \\\n",
       "681                          {'rubbers': 1, 'mins': 1}   \n",
       "412  {'warranty': 4, 'receipt': 1, 'troublesome': 1...   \n",
       "289  {'vengence': 1, 'gpus': 1, 'gpu': 2, 'setup': ...   \n",
       "272                                                 {}   \n",
       "308                                                 {}   \n",
       "\n",
       "                                 Token_Reply  Sentiment  \n",
       "681                            {'mathew': 1}     0.9008  \n",
       "412                          {'warranty': 2}    -0.7243  \n",
       "289                       {'regretfully': 1}     0.9961  \n",
       "272                                       {}     0.9509  \n",
       "308  {'lonlon': 1, 'apologize': 1, 'rma': 1}    -0.8172  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def compute_sentiment(a):\n",
    "    return sia.polarity_scores(a)['compound']\n",
    "\n",
    "df['Sentiment'] = df['Review'].apply(compute_sentiment)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5de4dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"TrustPilotData-Scrubbed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7867c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pool_dict = dict(pool)\n",
    "pooled_words = pd.DataFrame(pool_dict.items(),columns=[\"Word\",\"Frequency\"])\n",
    "pooled_words.to_csv(\"PooledWords.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
